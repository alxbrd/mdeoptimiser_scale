package uk.ac.kcl.inf.mdeoptimiser.infrastructure.scale.interpreter.experiments.tasks;

import com.amazonaws.services.batch.model.ContainerProperties;
import uk.ac.kcl.inf.mdeoptimiser.infrastructure.scale.jobs.ScaleJobResult;
import uk.ac.kcl.inf.mdeoptimiser.languages.scale.Model;

import java.io.File;
import java.nio.file.Path;
import java.util.List;
import java.util.Map;
import java.util.Properties;

public interface IScaleTask {

  /**
   * Helper field to idenfity the correct deserialisation instance type.
   * @return name of the class implementing this interface.
   */
  String getClassName();

  /**
   * Get the unique ID of the current task
   * @return id of the task
   */
  String getId();

  /**
   * This should return the type of the task using TaskType. Currently this can be MDEO or MOMoT.
   * @return value indicating the type of the current task
   */
  TaskType getType();

  /**
   * Get the name of the task as configured in the DSL.
   * @return name of the task
   */
  String getName();

  /**
   * Get model name
   * @return mode name
   */
  String getModelName();

  /**
   * The classpath dependencies of the current task. This currently considers only Java tasks.
   * //TODO what kind of dependencies do we have to support for other types of tools? Should we assume
   * they come bundled in the user provided container, where no additional libraries are required to implement
   * the fitness functions? Eg: MiniZinc/Essence
   * @return dependencies path for the tool used by this task
   */
  Map<String, File> getDependencies();

  /**
   * The path to the spec to be executed by this task. This includes the spec file specified by the user in the DSL
   * @return the tool spec
   */
  String getCommand();

  /**
   * TODO: Not sure about this function here. Maybe the validation should be done in the DSL?
   */
  boolean validateTask();


  /**
   * Returns a map of the experiment artifacts containing a mapping of the path on S3 to the equivalent paths stored locally.
   * @return
   */
  Map<String, File> getExperimentArtifacts();

  /**
   * Returns a map of the task files containing a mapping of the path on S3 to the equivalent paths stored locally.
   * @return
   */
  Map<String, File> getTaskFiles();

  /**
   * Generates a container properties user by the aws provider to configure job configurations.
   * This includes things like vCpus, memory, container image and entry command
   * @return properties file containing the container properties for the configured container.
   */
  Properties getContainerProperties();

  /**
   * Get the user configured experiment name
   * @return name of the experiment configured in the DSL
   */
  String getExperimentName();

  /**
   * The root path where the experiment artifacts and files are stored on S3. This path contains the global experiment
   * files configured in the DSL. Inside this location the specific artifacts for each model in a configured
   * experiment, are uploaded.
   * @return root path of the experiment artifacts on S3
   */
  String getExperimentInstanceNamePrefix();

  /**
   * The root path where the experiment model artifacts and files are stored on S3. This path is below the experiment instance
   * name prefix and contains artifacts that are specific for a configured model instance, part of an experiment configuration.
   * @return path where the experiments are stored for the current model inside the experiment configured for this task.
   */
  String getExperimentModelNamePrefix();


  // Executable tool interface methods

  /**
   * Get the configured tool name
   * @return name of the current tool configured for this task.
   */
  String getToolName();

  /**
   * This method gets executed by the service wrapper inside the provided container for the configured tool.
   * @return job result instance containing paths to the generated result artifacts
   */
  ScaleJobResult run(int batchNumber);

  /**
   * Return the paths where the result files have been generated by the tool.
   * @return list containing all the paths containing artifacts produced by the tool following a run.
   */
  List<Path> getResultLocations();

  /**
   * Get the path to the collected accumulator for the current job run.
   * @return a path pointing to the accumulator of the current run.
   */
  Path getResultsAccumulator();

  /**
   * Get results upload key
   * @return an aws prefix where the results file for a given batch is uploaded
   */
  String getBatchResultsKey(int batchId);

}